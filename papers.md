|   年份 | 方法名称 | 包含记忆类型 | memory存储方法 | 记忆遗忘机制 | 主要应用场景 | 记忆形成机制 | 记忆检索机制 | 记忆更新机制 | 记忆表示粒度 | 评估指标 | 关键技术依赖 | 创新性 | 主要局限 |
|---:|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|
| 1 | VIMBANK | 长期记忆 | 向量数据库 | 动态更新策略, 艾宾浩斯遗忘曲线理论 | 多任务决策优化（如对话、任务规划） | 历史交互数据通过向量数据库存储，结合动态更新策略强化关键信息 | 向量相似度检索（Cosine Similarity） | 动态更新策略，根据任务需求更新关键信息，淘汰不重要数据 | 任务状态,上下文信息 | 任务决策成功率、 | EbbdingHaus遗忘曲线理论, 动态更新策略, 向量数据库 | 结合动态更新策略与遗忘曲线理论，优化存储和推理成本 | 存储和检索开销较高，需依赖高效的向量数据库 |
| 2 | SAGE | 短期记忆, 长期记忆 | 知识图谱 | 动态优先管理, 艾宾浩斯遗忘曲线理论 | | 通过反射（Reflection）生成关键信息，结合任务历史动态调整 | 图结构中的语义关联检索 | 根据任务完成情况更新记忆，淘汰冗余信息 | 任务状态,上下文信息 | 任务成功率, 基准测试性能 | EbbdingHaus遗忘曲线理论, 反射机制, 知识图谱数据库 | 引入反射机制和Ebbinghaus曲线优化，支持小模型显著提升性能 | 存储和检索开销较高，需依赖图结构的高效管理 |
| 3 | MaLP | 短期记忆, 长期记忆 | 向量数据库, 知识图谱 | 基于频率计数（Frequency Counting） | 医疗助手个性化（如患者偏好、历史记录） | 通过对话生成记忆并分类为常识（Common-sense）和用户特定（User-specific） | 最近邻检索（STM）, 语义匹配（LTM） | 定期刷新工作记忆（Working Memory），LTM仅接受新条目 | 键值对（Key-Value Pairs） | BLEU, ROUGE, 准确率 | LoRA(参数高效微调）, 向量数据库, 知识图谱数据库 | 提出DPeM机制（双过程记忆），结合PEFT实现个性化 | 离线存储、无法学习新查询，隐私问题 |
| 4 | LONGMEM | 长期记忆 | 分块, 向量数据库 | | 长上下文语言建模（如书籍、论文） | 缓存历史上下文（Past Context）并通过分块存储 | Token-to-Chunk检索（减少索引规模） | 动态扩展内存银行（Memory Bank） | 分块（Chunk）大小为csz（默认64） | 混淆度（Perplexity） | 侧网（SideNet, 分块检索, 解耦网络架构 | 解耦架构（冻结主LLM + 侧网）支持无限长上下文 | 模型复杂度高，依赖分块策略 |
| 5 | Reflexion | 短期记忆, 长期记忆: 存储自我反思 | 向量数据库 | 基于反馈（Feedback）淘汰错误记忆 | 对话系统（如任务导向对话） | 反射生成文本记忆，存储为轨迹（Trajectory） | 向量相似度检索（Cosine Similarity） | 根据环境反馈更新记忆，淘汰无效信息 | 轨迹（Trajectory） | 任务成功率 | 反射机制, 向量数据库 | 反射机制生成经验，支持错误修正 | 依赖高质量反馈，无法处理复杂环境 |
| 7 | TiM | 中期记忆, 长期记忆 | 原始文本, 向量（LSH哈希表） | 移除矛盾或冗余的想法（ | 医疗对话（如疾病诊断）。 | 动态更新记忆，通过LLM生成思想并分类到哈希表中。 | 基于LSH快速检索相似历史思想，结合上下文关联性。 | 插入新思想、合并相似思想（如相同实体的多条记录）。 | 以思想（thought）为单位，粒度较细。 | 准确率, 响应正确率, 对话连贯性（Human Evaluation） | LLM生成, LSH哈希 | 提出动态记忆更新和LSH加速检索，解决长对话中的遗忘问题。 | 依赖LLM生成高质量思想，长对话场景下检索效率可能受限。 |
| 8 | RAP | 中期（任务轨迹）, 长期记忆 | 向量数据库（存储成功任务的轨迹）。 | 保留成功任务的经验实现隐式遗忘。 | WebShop（电子商务）、ALFWorld（虚拟任务）。 | 记录任务轨迹（计划、动作、观察），通过递归执行生成经验 | 基于相似动作检索历史经验，提取相关轨迹和动作。 | 通过成功任务扩展记忆数据库，递归试错（最大深度3）。 | 任务轨迹（动作序列和观察） | 任务成功率, 总奖励分数 | LLM生成, RAG | 结合任务与场景的关系（A是B如同C是D），提升跨任务泛化能力 | 依赖任务描述的准确性，对非结构化环境适应性有限。 |
| 9 | LOCOMO | 中期（单会话）, 长期（多会话 | 会话摘要, 向量数据库 | 通过摘要压缩历史信息 | 多模态对话生成（如文本+图像）。 | 生成对话历史摘要，结合事件图谱（Temporal Event Graph） | DRAGON检索相关上下文, 会话摘要 | 迭代生成摘要，逐步扩展对话历史。 | 事件（Event）,会话段（Session | F1分数, FactScore, MMRelevance | DRAGON检索, LLM生成 | 构建LOCOMO数据集（300轮对话），提出事件图谱驱动的多模态对话生成。 | 数据集依赖人工标注，评估指标需更多主观验证。 |
| 10 | NADINE | 中期（对话历史）, 短期（实时交互）, 长期（用户画像） | 情景记忆（Episodic Memory）, 知识图谱 | 情景记忆的动态更新实现隐式遗忘。 主要应用场景 | 社交机器人（如Nadine平台）。 | 通过多模态输入（语音、骨骼检测）构建用户画像和情景记忆 | 图结构中的语义关联检索, 情景关联性 | 动态更新用户画像和情景记忆，结合情感状态。 | 用户行为（Behavior）,情景事件 | 人类评分, 对话连贯性（Human Evaluation）, 总奖励分数 | RAG, 情感分析, 知识图谱数据库 | 集成情感和记忆模块，实现类人社交交互。 | 情感识别依赖外部模型（Google STT），复杂场景下表现不稳定。 |
| 11 | RAISE | 中期（对话历史）, 短期（实时交互） | 原始文本, 向量数据库 | 场景分割隐式过滤无关信息 | 房地产咨询（如在线IM对话）。 | 结合场景提取和CoT（链式推理）生成记忆。 | 基于当前查询和历史场景匹配示例。 | 微调LLM以适应场景，增量更新记忆库。 | 会话段（Session,情景事件 | 响应质量评分, 推理效率 | LLM生成, LoRA(参数高效微调） | 提出RAISE框架，结合示例和草稿提升对话代理可控性。 | 依赖高质量训练数据，复杂逻辑问题处理能力有限 |
| 12 | Agentic-RAG | 中期（窗口序列）, 长期（历史数据） | 向量数据库, 表格（时间戳+特征） | 固定窗口滑动隐式丢弃旧数据 | 交通预测（PeMSD3/PeMSD7）、异常检测。 时间序列分析，包括但不限于分类、异常检测、预测、插补等任务。 | 时序窗口分割+向量嵌入（all-mpnet-base-v2）。 | 元数据分类, 向量相似度检索（Cosine Similarity） | 动态更新窗口，量化模型（QLoRA）优化存储。 | 时间窗口（32k序列),事件（Event） | F1分数（异常检测）, MAE/RMSE/MAPE(预测误差） | LLM生成, LoRA(参数高效微调）, 向量数据库 | 结合结构化数据（表格）与向量检索，提升时序任务性能。 | 对非平稳时间序列适应性不足，依赖高质量标注数据 |
| 13 | EM-LLM | 短期（当前事件）, 长期（事件分割） | 向量数据库, 知识图谱 | 过滤低相关性片段 | 长上下文任务（LongBench/∞-Bench）。 | 通过注意力相似性构建事件图，结合惊喜度分割事件。 | 分层k-NN检索, 时间连续性 | 动态调整事件边界，优化图结构（Santo Fortunato社区检测）。 | 事件（Event）,子事件 | F1分数, 准确率, 召回率 | LLM生成, 事件分割算法（Bayesian Surprise）, 注意力机制 | 提出基于惊喜度的事件分割，模仿人类记忆形成机制。 | 计算复杂度较高（O(kn)），依赖LLM的注意力质量。 |
| 14 | Optimus | 中期（AMEP, 长期（HDKG） | 分层有向知识图谱, 向量（抽象多模态经验池）。 | AMEP动态总结经验，过滤冗余信息。 | Minecraft任务（如合成工具）。 | 通过视频帧和图像缓冲区提取多模态经验，构建HDKG。 | 知识图谱路径检索, 视觉相似性（MineCLIP） | 非参数学习（自由探索-教师指导），增量更新HDKG和AMEP。 | 物品关系,视觉帧 | FDR（故障检测率）, 任务成功率 | HDKG构建, MineCLIP, 多模态LLM | 提出HDKG+AMEP混合记忆，支持多模态任务规划与反思 | 依赖特定环境（Minecraft），泛化能力需验证。 |
| 15 | MEMLLM | 短期记忆, 长期记忆 | 关系数据库, 向量索引 | 上下文刷新, 按需替换 | 知识编辑、问答系统、对话系统、文档级关系提取等 | 通过处理文本或与用户交互动态写入信息到内存，以关系三元组形式存储 | 微调驱动的“记忆读取 | 通过“记忆写入”进行动态更新 | 关系三元组 | 可靠性(REL), 对话连贯性（Human Evaluation）, 局部性(LOC), 泛化能力(GEN), 混淆度（Perplexity） | 关系数据库, 向量嵌入 | 显式的内存结构允许大规模的知识编辑，同时提高了模型输出的可解释性和准确性 | 对于大量连续编辑操作时性能可能下降；依赖高质量的数据和预训练模型 |
| 16 | SECOM系统 | 短期（实时交互）, 长期记忆 | 向量数据库 | 通过检索实现隐式遗忘 | 长程个性化对话智能体 (Long-term Personalized Conversational Agents): 使机器人在长时间、多主题的对话中保持上下文感知和个性化 。 | 分割 & 压缩 (SEgmentation & COMpression) • 分割: 使用一个分割模型（如GPT-4）将原始的长对话历史按主题切分成连贯的“对话片段” 。 压缩/去噪: 在存入记忆库前，使用提示压缩模型 | 主题片段, 向量相似度检索（Cosine Similarity） | 基于历史构建 | 主题片段 | BLEU, ROUGE, 成对比较 (Pairwise Comparison | LLM生成, RAG, 提示压缩模型 | | |
| 17 | MRSTEVE系统 | 中期（任务轨迹）, 短期（实时交互）, 长期记忆 | 关系数据库, 向量数据库, 时序数据库, 知识图谱 | - FIFO淘汰：当内存满时，移除最大聚类中最旧的帧, 任务完成后的动态更新（B任务周期）, 压缩式去噪 | 长期任务解决：如获取钻石需经过找矿石、制作铁镐等多步骤。<br>- 开放环境探索：在稀疏资源分布的地图中高效定位目标（如“找到牛”）。<br>- 多任务序列：处理A-B-A任务（如“木材→树叶→木材”）。 | 事件聚类：通过MineCLIP计算帧嵌入相似度，使用DP-Means聚类生成事件簇（Event Clusters）。<br>- 位置聚类：基于位置坐标和朝向划分区域（Place Clusters），每个区域独立维护事件。<br>- 动态更新：每100帧触发聚类更新，合并相似事件，确保关键信息保留。 | 分层k-NN检索, 向量相似度检索（Cosine Similarity） | 增量更新：每100帧更新聚类（R=100），合并新事件到现有簇或创建新簇。<br>- 动态扩展：内存容量不足时，优先删除最大簇中最旧的帧。<br>- 探索驱动：通过Count-Based探索策略动态调整目标位置，更新记忆中的未探索区 | 视觉帧，场景帧，区域帧 | 任务成功率 | | PEM分层结构：首次将位置与事件联合建模，支持高效检索和导航。<br>- 分层探索策略：结合Count-Based目标选择和VPT-Nav导航，减少冗余探索。<br>- 事件-位置联合存储：通过聚类解决传统FIFO记忆的低效问题，提升长序列任务表现。 | 资源受限环境：在内存容量极低时（<0.1K帧），PEM性能下降（如MrSteve-FM在Beef-Log-Beef任务中失败率高）。<br>- 复杂地形依赖：VPT-Nav在极端地形（如悬崖）表现不佳。<br>- 噪声位置适应性：依赖精确位置数据，难以迁移到机器人任务（如噪声位置输入 |
| 18 | AgentCF++ | 中期（单会话）, 短期（实时交互）, 长期记忆 | 向量数据库, 向量数据库（存储成功任务的轨迹）。 | 通过定期重新划分兴趣组来动态调整和优化记忆内容。 | 跨域推荐系统中，模拟用户的交互行为以提高推荐准确性 | 初始时使用物品的辅助信息填充记忆；随后通过用户与物品代理之间的互动逐步构建和细化。 | | 反思机制用于更新用户和物品的记忆，包括根据最新互动学习偏好，并通过两步融合机制整合跨域信息。 | 兴趣组内的集体行为模式 | MRR, NDCG | K-mean, LLM生成, 协同过滤技术 | 提出了一种双层记忆架构和两步融合机制，有效地结合了跨领域的偏好信息，同时引入了基于兴趣的群体和群体共享记忆概念 | 主要集中在如何更精细地区分不同用户的兴趣群体，避免不相关的流行因素对无关用户的干扰 |
| 19 | A-MEM | 中期记忆, 短期记忆, 长期记忆 | 关系数据库, 向量数据库 | 任务完成后的动态更新（B任务周期） | 主要应用于问答任务（QA），包括单跳、多跳、时间性、开放领域和对抗性问题的回答 | 系统处理新的交互记忆并将其作为带有多个属性的笔记存储下来。然后使用LLM来决定是否应建立这些记忆之间的连接。 | 图结构中的语义关联检索 | 记忆可以通过链接生成和记忆进化过程不断更新，以反映新的交互和知识。 | 事件（Event）,时间戳 | F1分数, ROUGE | LLM生成, 关系数据库, 向量数据库 | A-MEM提出了一种自我演化的记忆系统，它能够自主地生成上下文描述，并与相关记忆建立连接，从而形成了一个动态的知识网络。 | 尽管A-MEM展示了强大的性能，但其复杂性和依赖于高级语言模型的特点可能会限制其在资源有限环境下的应用。此外，对于非常大规模的数据集，如何有效管理记忆连接仍是一个挑战。 |
| 20 | CDMem | 短期记忆编码, 长期记忆编码 | 知识图谱 | 通过检索实现隐式遗忘 | 交互式决策制定基准测试，如ALFWorld和ScienceWorld，在导航和操作领域 | • 多阶段记忆编码策略，包括专家编码、短期记忆编码和长期记忆编码，分别对应不同层次的知识获取。 | 向量相似度检索（Cosine Similarity）, 语义匹配（LTM） | • 在线学习架构，使代理能够有效地学习并更新记忆，同时适应现实世界应用中的新环境和任务。 | 轨迹（Trajectory）,任务轨迹（动作序列和观察） | F1分数, 任务成功率 | LLM生成 | • 提出了一种受人类记忆机制启发的在线记忆范式，特别是适合工业应用中开发特定领域的代理。 | 需要更多的API调用，增加了计算成本；在某些环境中，去除特定组件（如环境洞察或任务洞察）可能会显著降低性能 |
| 21 | MINJA | 短期记忆, 长期记忆 | 向量数据库, 表格（时间戳+特征） | 基于反馈（Feedback）淘汰错误记忆 | 包括但不限于：自主驾驶、金融、医疗保健、代码生成和网络任务等。 | 当新的查询出现时，从记忆库中检索最相关的LTM记录作为有效执行任务的示范。此外，通过用户的反馈进一步改进系 | 向量相似度检索（Cosine Similarity） | 用户反馈决定了记录是否会被存储到记忆库中，这意味着只有正确执行的查询才会被保存（例如，在RAP上进行网络购物）。而对于EHRAgent和QA Agent，所有执行记录都会被存储。 | 思考推理步骤 | 攻击成功率, 注入成功率 | | 提出了MINJA，一种针对LLM代理的记忆注入攻击，仅通过查询即可向代理的记忆库中注入特制的恶意记录 | 击者不能直接修改记忆或干扰受害者用户的查询，需要诱导代理自动生成并存储恶意记录 |
| 22 | AriGraph | Episodic Memory (情景记忆), Semantic Memory (语义记忆 | 关系数据库, 向量数据库 | 通过检索实现隐式遗忘 | 包括Treasure Hunt、Cleaning、Cooking等任务，这些任务测试代理在复杂环境中的导航能力、物品管理以及遵循指令的能力 | • 在每次观察时，Ariadne代理会更新其世界模型，并从AriGraph检索相关的情景和语义知识到工作记忆。• 工作记忆还包括最终目标描述、当前观察、最近观察和行动的历史记录。• 规划模块使用工作记忆的内容创建或更新计划，该计划是一系列与任务相关的子目标，每个子目标都有简明的描述。 | 语义匹配（LTM） | 根据环境反馈调整计划，计划更新后会被加回到工作记忆中，由决策模块选择最合适的行动。 • 用户反馈决定了记录是否会被存储到记忆库中，这意味着只有正确执行的查询才会被保 | | 任务成功率 | LLM生成 | • AriGraph通过学习情景和语义知识并在未知环境中进行互动，展示了良好的可扩展性。 • 结合ReAct框架，允许代理在执行动作前阐明其理由，从而支持更精细的决策过 | 尽管AriGraph在各种任务中表现出色，但在处理非常复杂的环境或需要高度特定领域知识的任务时，可能仍然存在挑战 |
| 23 | LONGMEM | 长期记忆键值对 | 键值对 | | | | | | | | | | |
| 24 | CAIM（Cognitive AI Memory Framework） | 中期记忆, 短期（当前事件）, 长期记忆, 长期（多会话 | 信息组织成层次化的本体结构 | 去除过时的数据来优化响应准确性的重要性。 | • 长期的人工智能与人类交互，特别是在需要适应用户偏好变化和个人化响应的情境中。 | • 通过对话结束后提取关键事件，将其转化为简短的归纳思想，并添加相关标签和时间戳来形成记忆。 | 上下文敏感的时间线精炼, 标签匹配 | • 包括Memory Extension和Memory Review两个阶段，前者用于存储新数据，后者用于合并重复条目并保持内存的一致性和最新状态。 | 单个词语标签,个归纳思想 | 准确率, 响应正确率, 对话连贯性（Human Evaluation） | 分层结构, 时间戳匹配 | • 结合认知AI原则，提供了一种更加人性化、上下文感知的记忆机制，提高了LLM在长期内与人类互动的能力。 | • 虽然解决了部分现有模型的问题，但在实际应用中可能仍受限于计算资源、数据隐私等问题。 |
| 25 | CarMem | 中期记忆, 短期记忆, 长期记忆 | 向量数据库 | 一致性检查，实现增加跳过还是更新 | 主要是针对车内语音助手设置，例如娱乐系统中的首选电台等 | 基于用户与助理之间的对话，利用大型语言模型（LLMs）从对话中捕捉偏好，同时忽略不在类别模式中的主题 | 向量相似度检索（Cosine Similarity） | • 当新偏好被识别时，会与现有的偏好进行对比，以决定是否需要更新、追加或跳过。 记忆表示粒度 | | F1分数, 准确率 | Faiss索引, LLM生成 | 提出了一种通过限定在预定义类别内的偏好记忆系统，提高了透明度并允许用户控制哪些偏好会被初始提取 | 局限性: 尽管系统实现了对偏好的有效管理和检索，但在处理非常大规模的数据时可能会遇到挑战，此外，对于非结构化数据的处理仍是一个未充分探索的领域 |
| 26 | ChatTs | | | | | | | | | | | | |
| 27 | Crafting Personalized Agents | 短期记忆, 长期记忆 | 知识图谱 | 一致性检查，实现增加跳过还是更新 | 创建个性化代理，适用于客户服务、个人助理等需要长期记忆和上下文理解的应用场景。 | 基于用户对话中提取的偏好和其他相关信息，系统能够动态地构建用户的个人记忆库。使用GPT-4从原始数据中清理并生成QA对。 | 图结构中的语义关联检索, 强化学习，自适应选择 | 当新偏好被识别时，系统会比较现有偏好并决定是否追加、跳过或更新。利用RL优化记忆选择过程。 | | BLEU, F1分数, ROUGE, 准确率 | Faiss索引, RAG, 图嵌入（TransE, 神经网络 | 提出了一种新的记忆管理系统，提高了用户交互体验的个性化程度。引入了可编辑的记忆图(EMG)和强化学习驱动的选择机制。 | 在处理大规模非结构化数据方面可能存在挑战；具体使用的底层数据库技术不明确。 |
| 28 | EHC | 中期：动态迁移池, 短期：工作记忆缓冲区（RAM）, 长期：外部数据库（按任务分类存储） | - 关系数据库（SQL，存储轨迹）, - 内存数据库（Redis，快速访问池）, - 向量数据库（相似性检索） | 据相关性/时效性评分（$rank_score=recency+relevance$）选择Top-K记忆 | 多模态任务处理（GQA视觉问答、NLVR2推理、MagicBrush图像编辑） | 试错收集轨迹（$T$次尝试）2. LLM+BERT分类经验3. 跨类别对比学习（成功/失败模式提取） | 任务类别过滤, 向量相似度检索（Cosine Similarity） | - 成功轨迹直接存储- 失败轨迹存储反思链（$r_{t+1}=\text{concat}(r_t,\text{LLM}(\cdot)$）- 动态迁移策略（容量阈值$C$触发 | 任务轨迹（动作序列和观察）,单个词语标签 | BLEU, 准确率 | 1. LRU淘汰算法, 2. BERT语义相似度, 3. 组合式视觉推理框架（VisProg） | 1. 双池内存架构（Fast-Access + Deep-Retrieval）2. 任务类别导向学习（TOEL）3. 跨类别对比洞察（$C^{\text{category}}_{\text{compare}}$ | . 依赖预定义任务类别2. 未测试超长对话（>200轮）3. 简单任务提升有限（如标记任务+0.06% |
| 29 | Self-Controlled Memory, SCM | 短期：闪存（Flash Memory，前序片段）, 长期：深层记忆（月/年级数据）, 长期：激活记忆（Activation Memory，历史事件） | - 文本摘要库（自然语言记忆项）, - 时序数据库（记录交互索引）, 向量数据库 | - FIFO淘汰：当内存满时，移除最大聚类中最旧的帧 | 长文本处理（长时对话、书籍摘要、会议摘要） | 1. 分段处理输入2. 控制器生成摘要（图3提示词）3. 嵌入存储（text-embedding-ada-002） | 1. 控制器决策是否激活记忆（图5提示词）, 3. 摘要替代原始内容（若token>800）, 向量相似度检索（Cosine Similarity） | - 用户交互自动入库- 记忆权重动态调整（初始权重+反馈更新）- 低权重记忆自动删除 | 交互三元组：{观察 响应 摘要} | 人类评分, 准确率, 召回率 | 1. text-embedding-ada-002, 3. 递归摘要技术, LLM生成 | 1. 自控工作流（6步流程）2. 记忆摘要可替代性决策3. 无限长度文本处理（无需微调） | 1. 需强指令遵循LLM（如davinci-003）2. 摘要可能丢失细节（图15示例）3. 中文场景验证不足 |
| 30 | FINMEM | 中期：浅层/中层记忆（日/周级数据）, 短期：工作记忆（动态工作区）, 长期：深层记忆（月/年级数据） | - 关系数据库（PostgreSQL，结构化存储事件）, - 内存数据库（Redis，工作记忆）, - 向量数据库（FAISS，相似性检索） | + 重要性阈值过滤（Impact Score < θ）, 分层衰减因子 | 金融交易决策（股票/期货） | 1. 事件加权编码（时效性×重要性）2. 分层存储（日新闻→浅层；季报→深层）3. 跨层迁移（高Impact事件下沉到深层） | . 分层加权聚合：$S = ω_1\cdot recency + ω_2\cdot relevance + ω_3\cdot importanc, 1. 工作记忆实时检索（Redis）, 3. Top-K事件融合决策 | 事件降级：低重要性事件向浅层迁移- 跨层升级：高Impact事件下沉深层（Impact Score > τ）- 衰减淘汰：定期清理低分事件 | 金融事件单元：{事件类型，时间戳，影响分数，原始文本} | 夏普比率, 年化收益率 | 3. 衰减因子计算模型, Faiss索引, LLM生成 | 1. 认知对齐的分层记忆（浅/中/深层）2. 动态风险画像（自适应风险偏好）3. 事件影响分数（量化跨层迁移） | 1. 依赖金融数据时效性标注2. 未整合实时行情接口3. 黑盒决策风险（LLM逻辑不透明 |
| 31 | FireAct | 中期记忆, 短期记忆, 长期记忆 | 轨迹数据集 存储ReAct格式的推理-动作-观察序列 | 任务完成后的动态更新（B任务周期） | 开放域问答多跳推理、知识检索HotpotQA, Bamboogle（§4）工具增强推理结合Google Search API解决复杂问题ReAct轨迹中的search[query]（§3）多任务泛化单一模型适应多样化QA任务混合HotpotQA/StrategyQA/MMLU数据（§7） | 轨迹蒸馏，机制实现方式论文技术轨迹蒸馏GPT-4生成轨迹 → 微调小模型知识蒸馏（Hinton et al. 2015）（§3）多方法混合融合ReAct/CoT/Reflexion轨迹 | 模型自适应选择推理策略（CoT/ReAct） | | 轨迹（Trajectory）,token | 准确率, 推理速度 | | | |
| 32 | MEMTRee。 | 长期记忆 | 动态树结构（非传统数据库），节点包含文本内容、语义嵌入及层级关系。, 向量数据库 | 任务完成后的动态更新（B任务周期） | 多轮对话理解（Multi-Session Chat）；- 单文档问答（QuALITY）；- 多文档多跳问答（MultiHop RAG），尤其擅长时间序列推理、跨文档比较等复杂任务。 | 新信息从根节点开始遍历树结构，通过语义嵌入相似度（余弦相似度）判断：若与子节点相似度超过深度自适应阈值，则深入该子节点；否则创建新叶节点。同时更新路径上所有父节点的内容（聚合现有内容与新信息）和嵌入，形成层次化结构。 | 将所有节点视为扁平集合，计算查询嵌入与节点嵌入的余弦相似度 | - 插入复杂度 O (log N)，从根节点遍历，根据相似度决定插入位置；- 父节点内容通过 LLM 聚合函数更新（结合新信息与现有内容，随子节点数量增加更抽象）；- 嵌入更新为聚合后内容的新嵌入，支持 CPU 并行更新以提升效率 | 层次化粒度：浅层节点（根附近）存储抽象概括信息，深层节点存储具体细节，深度越大信息越具体（如深度 10 以上节点平均 tokens 达 800） | 准确率, 召回率 | Faiss索引, LLM生成, 余弦相似度计算, 在线层次聚类理论 | 1. 动态树结构模拟人类认知 schema，支持在线更新（无需重建），兼顾结构化为与灵活性；2. 深度自适应阈值与父节点聚合机制，实现层次化语义整合；3. 结合在线层次聚类理论，提供近似最优性保证 | . 树结构灵活性低于图结构，处理非层次化关系可能不足；2. 依赖嵌入模型和 LLM 的聚合能力，性能受限于这些模型；3. 未解决主动遗忘机制，长期积累可能导致冗余；4. 复杂查询可能因节点深度过深导致检索效率下降 |
| 33 | RMM | 不分长/中/短期，统一存储为（主题摘要，原始对话）对 | 主题摘要作为检索键，原始对话存储为向量 | | 个性化医疗代理、客服、教育平台 | 前瞻性反射：1. 会话结束时按主题提取片段2. 与现有记忆合并或新增（LLM决策 | 1. 检索Top-K记忆, 2. 可学习重排序器（Gumbel采样）→ 筛选Top-M, 引用证据反馈优化 | 回溯性反射：- LLM生成响应时标注引用证据- 基于引用信号（+1/-1）强化学习更新重排序器 | 动态主题粒度： - 话语（utterance）→ 对话轮次（turn）→ 会话（session）→ 跨会话主题 | BERTScore, METEOR, 准确率, 召回率 | Gumbel-Softmax重排序, LLM生成, 向量数据库 | 1. 多粒度主题记忆组织2. 无监督RL检索优化（基于LLM引用信号） | 1. RL训练计算开销大2. 仅支持文本模态3. 未处理记忆冲突 |
| 34 | mem0 | - 中长期：会话摘要 + 图结构记忆（实体关系）, - 短期：最近消息（10条） | - Mem0：向量数据库（自然语言记忆）, - MemO<sup>S</sup>：Neo4j图数据库（实体节点+关系边） | - 冲突时标记旧关系为""过时""（非物理删除）, ✅ 时间戳驱动： | 个人助理、医疗/教育领域、企业支持系统 | 增量提取：1. 新消息对 + 会话摘要 → LLM提取候选记忆2. 冲突检测 → LLM决策操作（ADD/UPDATE/DELETE/NOOP） | 1. 实体中心：锚点节点扩展子图, 语义三元组：查询向量匹配关系编码, 阈值筛选+排序 | 操作分类器：- LLM直接决策操作类型（ADD/UPDATE/DELETE/NOOP）- 基于语义相似度与冲突检 | 混合粒度：- Mem0：自然语言事实（句子级）- MemO<sup>S</sup>：实体（节点）+ 关系（边） | F1分数, 准确率 | LLM生成, 时间戳匹配, 知识图谱数据库 | 1. 异步摘要生成模块2. 图记忆的冲突检测与时间推理3. 混合检索架构（实体+语义 | 1. 图构建增加延迟（MemO<sup>S</sup>）2. 开放域任务弱于Zep3. 依赖LLM生成操作决策 |
| 35 | MemInsight | 长期记忆键值对 | - 属性键值对直接存储, 向量数据库 | 过滤低相关性片段 | 会话推荐、问答、事件摘要 | 自主属性挖掘：- LLM生成实体/会话中心属性（如电影类型、用户意图）- 粒度分Turn/Session级 | 向量相似度检索（Cosine Similarity）, 属性检索：精确匹配属性键值对 | 新交互触发属性重标注- 优先级排序（高相关属性优先 | Turn级：单轮对话细粒度属性- Session级：整个会话概括属性 | 召回率, 响应正确率, 对话连贯性（Human Evaluation） | Faiss索引, LLM生成, 向量数据库 | 自主属性生成：LLM自主挖掘语义属性，替代人工设计结构 | 依赖LLM标注质量（可能幻觉）未解决记忆爆炸问题 |
| 36 | MEOS | - 中期(MTM)：主题分段+对话页面, - 短期(STM)：对话页面（含元数据链）, - 长期(LPM)：用户画像+动态知识库 | - 分段分页存储（类似OS内存管理）, - 知识库（用户画像/动态特征）, 向量数据库, 知识图谱 | 显式热度衰减： - 热度公式：$Heat = \alpha N_{visit} + \beta L_{interaction} + \gamma R_{recency}$ - 低热度分段从MTM移除 | 超长对话（300+轮）、个性化交互（如健身提醒、兴趣追踪） | 动态分层存储：- STM→MTM：FIFO队列- MTM→LPM：热度阈值触发- LPM更新：FIFO队列 | 1. MTM分段语义筛选（Top-M）, 2. 分段内页面相似度检索（Top-K）, 3. LPM画像直接调用 | STM实时更新- MTM分段合并/拆分（相似度阈值 $\theta=0.6$）- LPM按FIFO替换旧条目 | 页面级：单轮对话- 分段级：同一主题多轮摘要- 画像级：用户长期特 | F1分数（异常检测）, 准确率, 召回率, 对话连贯性（Human Evaluation） | - 画像维度设计（90维用户特征）, LLM生成, 向量数据库 | OS式分层管理：- 分段分页存储- 热度驱动更新- 多级检索协同 | 实现复杂度高热度公式需调参（$\alpha,\beta,\gamma$）LPM容量固定（FIFO可能丢弃重要信息） |
| 37 | MemoryBank | 短期记忆, 长期记忆：分层存储（原始对话+事件摘要+用户画像） | 向量数据库 | 艾宾浩斯遗忘曲线理论 | • 情感陪伴（如AI伴侣）• 心理咨询• 秘书任务 | 自动摘要（LLM生成事件/画像摘要）• 时序存储（带时间戳 | langchan集成, 双塔稠密检索 | • 动态更新画像（每日画像→全局画像）• 遗忘低强度记忆（$S$值衰减 | 三级粒度：1. 原始对话2. 事件摘要3. 用户画像（性格/情感） | 准确率, 响应正确率, 对话连贯性（Human Evaluation） | Faiss索引, LoRA(参数高效微调）, longchan | • 首个人类遗忘曲线启发的LLM记忆机制• 多模型兼容（开源/闭源） | • 依赖外部检索库• 用户画像仅基于显性对话（忽略隐式偏好 |
| 38 | MemSim | 评估对象：覆盖短/中/长期记忆（通过噪声比例控制时效性） | 关系数据库, 向量数据库 | 制作噪声干扰, 无遗忘 | 个人助理记忆能力评估• 多场景记忆可靠性测试 | • 贝叶斯关系网络生成结构化用户画像• 因果提示生成消息-QA对 | • LLM直接检索, 上下文敏感的时间线精炼, 向量相似度检索（Cosine Similarity） | 无主动更新机制• 依赖外部存储系统更新 | 两级粒度：1. 实体级（如人物/事件）2. 属性级（如年龄/职业 | F1分数, 响应正确率, 对话连贯性（Human Evaluation） | Faiss索引, 因果推断, 贝叶斯 | • 贝叶斯关系网络生成可靠测试数据• 因果生成解决LLM幻觉问题• 首个自动记忆评估框架 | 仅评估事实性记忆（不处理抽象概念）• 未涵盖对话场景评估 |
| 39 | TME | - 中期：活跃子树路径, - 短期：当前节点状态, - 长期：完整任务树（TMT） | - 未来支持图数据库（DAG）, - 树状结构（JSON序列化） | - 子树合并（语义相似节点）, - 路径剪枝（失效分支）, 主动修剪： | 多步骤LLM代理任务（表单填写、旅行规划） | 任务关系推理：- TRIM模块解析依赖/替换/回滚关系- 节点动态创建与更新 | - 从根节点到当前叶节点的路径遍历, - 动态提示合成（仅相关历史） | 节点级更新：- 状态标记（active/done/failed）- 输入输出动态写入 | 粗粒度：任务步骤级别（每个节点代表原子操作） | | LLM生成, json序列化, 树便利算法 | 结构化任务内存：- 树形状态跟踪- 动态提示合成- 轻量级关系推理（TRIM | 树结构限制跨分支依赖2. 依赖LLM关系分类精度 |
| 40 | STMA | - 中期：时空信念（压缩摘要）, - 短期：原始交互队列（FIFO）, - 长期：动态知识图谱 | - 关系数据库（空间关系三元组）, - 动态知识图谱（Neo4j类图数据库）, - 时序缓冲队列（内存） | - 历史缓冲队列（FIFO淘汰）, - 知识图谱边缘衰减（低频关系淘汰） | 具身智能长期任务（TextWorld烹饪、空间探索） | 时空摘要：- 时序摘要器压缩历史- 关系抽取器构建图谱- 空间聚合器生成自然语言描述 | - K-hop图谱搜索（实体关联扩展）, 向量相似度检索（Cosine Similarity） | 异步批量更新：- 时序缓冲实时追加- 图谱按步增量更新- 信念状态按需重构 | 细粒度：实体关系级别（KG三元组描述对象空间关系） | 任务成功率, 召回率 | 2. K-hop检索算法, 3. 轻量级Transformer（关系聚合）, 知识图谱数据库 | 时空联合建模：- 动态知识图谱- 规划-批判闭环架构- 开源模型高效适配（Qwen2.5） | 计算开销大（KG维护）2. 摘要器可能丢失细节3. 具身任务依赖仿真环境 |
| 41 | THEANINE | 中期：会话级记忆图, 短期：当前对话上下文, 长期：时间线记忆（因果链） | 向量数据库, 图数据库（存储因果/时序关系） | 保留所有记忆 | 多轮开放域对话（个性化代理、临床咨询） | 1. 会话摘要 → 2. 因果/时序关系链接 → 3. 构建记忆图 | - 时间线检索：从记忆图提取完整事件链,- 时间线检索：从记忆图提取完整事件链 - 反事实增强检索（TeaFarm） | 动态图扩展：新记忆链接到关联记忆节点 | | | | | |
| 42 | TReMu | 短期：检索增强的上下文, 长期：时间线摘要（带时间戳） | Elasticsearch（关键词索引）, 向量数据库（时间感知摘要 | 无显式遗忘 | 多会话时序推理（事件时间锚定、间隔计算） | 1. 提取事件 → 2. 跨会话事件链接 → 3. 生成时间线摘要 | - 反事实增强检索（TeaFarm） | 神经符号推理：LLM生成Python代码执行时间计算 | 时间线增量更新：新会话摘要追加到时间线 | | | | |
